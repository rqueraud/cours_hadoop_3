{"paragraphs":[{"text":"%md\n\nCe notebook est très largement inspiré de: https://github.com/andfanilo/pyspark-tutorial","user":"anonymous","dateUpdated":"2020-01-03T15:16:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Ce notebook est très largement inspiré de: <a href=\"https://github.com/andfanilo/pyspark-tutorial\">https://github.com/andfanilo/pyspark-tutorial</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578062567302_1163625103","id":"20200103-144247_1796990481","dateCreated":"2020-01-03T14:42:47+0000","dateStarted":"2020-01-03T15:16:54+0000","dateFinished":"2020-01-03T15:16:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1008"},{"text":"%spark.pyspark\ndef sum(a, b):\n    \"\"\"\n    Return a + b\n    \"\"\"\n    return a + b\n    \nassert sum(1, 2) == 3\n    ","user":"anonymous","dateUpdated":"2020-01-03T14:45:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578062593673_866441061","id":"20200103-144313_2058655224","dateCreated":"2020-01-03T14:43:13+0000","dateStarted":"2020-01-03T14:45:21+0000","dateFinished":"2020-01-03T14:45:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1009"},{"text":"%md\nA partir d'ici, on suit le notebook : https://github.com/andfanilo/pyspark-tutorial/blob/master/2-novice/1-Initiation-RDD.ipynb","user":"anonymous","dateUpdated":"2020-01-03T14:57:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578063436357_-990651046","id":"20200103-145716_582053025","dateCreated":"2020-01-03T14:57:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1656","dateFinished":"2020-01-03T14:57:32+0000","dateStarted":"2020-01-03T14:57:32+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>A partir d&rsquo;ici, on suit le notebook : <a href=\"https://github.com/andfanilo/pyspark-tutorial/blob/master/2-novice/1-Initiation-RDD.ipynb\">https://github.com/andfanilo/pyspark-tutorial/blob/master/2-novice/1-Initiation-RDD.ipynb</a></p>\n</div>"}]}},{"text":"%spark.pyspark\n\nfrom pyspark.rdd import RDD\n\nsc.parallelize([1, 2, 3]).collect()\n\ndef rdd_from_list(sc, n):\n    \"\"\"\n    Return a RDD consisting of elements from 1 to n. \n    For now we assume we will always get n > 1, no need to test for the exception nor raise an Exception.\n    \"\"\"\n    return sc.parallelize(range(1, n+1))\n    \n\n\"\"\"\nGraded cell\n\n1 point\n\"\"\"\n# collect() method returns all elements in a RDD to the driver as a local list\nprint(rdd_from_list(sc, 10).collect())\n\nresult_rdd = rdd_from_list(sc, 3)\n\nassert isinstance(result_rdd, RDD)\nassert result_rdd.collect() == [1, 2, 3]","user":"anonymous","dateUpdated":"2020-01-03T14:56:49+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"}]},"apps":[],"jobName":"paragraph_1578062704211_-1990598944","id":"20200103-144504_1162212700","dateCreated":"2020-01-03T14:45:04+0000","dateStarted":"2020-01-03T14:56:49+0000","dateFinished":"2020-01-03T14:56:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1010"},{"text":"%md\n\nPart B - Classic Spark operations\nOperations\nRDDs have two sets of parallel operations:\n\ntransformations : which return pointers to new RDDs without computing them, it rather waits for an action to compute itself.\nactions : which return values to the driver after running the computation. The collect() funcion is an operation which retrieves all elements of the distributed RDD to the driver.\nRDD transformations are lazy in a sense they do not compute their results immediately.\n\nThe following exercises study the usage of the most common Spark RDD operations.\n\n.map() and flatMap() transformation\nThe .map(function) applies the function given in argument to each of the elements inside the RDD.\n\nThe .flatMap(function) applies the function given in argument to each of the elements inside the RDD, then flattens the list so that there are no more nested elements inside it.\n\nQuestion 1\nSuppose we have a RDD containing only lists of 2 elements :\n\nmatrix = [[1,3], [2,5], [8,9]]\nmatrix_rdd = sc.parallelize(matrix)\nThis data structure is reminiscent of a matrix.\n\nCreate an operation .op1() which multiplies the first column (or first coordinate of each element) of the matrix by 2, and removes 3 to the second column (second coordinate).\n\n","user":"anonymous","dateUpdated":"2020-01-03T15:07:39+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578063770088_-1082280858","id":"20200103-150250_1585400570","dateCreated":"2020-01-03T15:02:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1922","dateFinished":"2020-01-03T15:07:39+0000","dateStarted":"2020-01-03T15:07:39+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Part B - Classic Spark operations<br/>Operations<br/>RDDs have two sets of parallel operations:</p>\n<p>transformations : which return pointers to new RDDs without computing them, it rather waits for an action to compute itself.<br/>actions : which return values to the driver after running the computation. The collect() funcion is an operation which retrieves all elements of the distributed RDD to the driver.<br/>RDD transformations are lazy in a sense they do not compute their results immediately.</p>\n<p>The following exercises study the usage of the most common Spark RDD operations.</p>\n<p>.map() and flatMap() transformation<br/>The .map(function) applies the function given in argument to each of the elements inside the RDD.</p>\n<p>The .flatMap(function) applies the function given in argument to each of the elements inside the RDD, then flattens the list so that there are no more nested elements inside it.</p>\n<p>Question 1<br/>Suppose we have a RDD containing only lists of 2 elements :</p>\n<p>matrix = <a href=\"./1%2C3%5D%2C-%5B2%2C5%5D%2C-%5B8%2C9.html\">1,3], [2,5], [8,9</a><br/>matrix_rdd = sc.parallelize(matrix)<br/>This data structure is reminiscent of a matrix.</p>\n<p>Create an operation .op1() which multiplies the first column (or first coordinate of each element) of the matrix by 2, and removes 3 to the second column (second coordinate).</p>\n</div>"}]}},{"text":"%spark.pyspark\n\nsc.parallelize([[1,3], [2,9]]).map(lambda row: row[0]).collect()","user":"anonymous","dateUpdated":"2020-01-03T15:04:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578063832748_-1002144858","id":"20200103-150352_1191285339","dateCreated":"2020-01-03T15:03:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1997","dateFinished":"2020-01-03T15:04:04+0000","dateStarted":"2020-01-03T15:04:04+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 2]\n"}]}},{"text":"%spark.pyspark\n\n\ndef op1(sc, mat):\n    \"\"\"\n    Multiply the first coordinate by 2, remove 3 to the second\n    \"\"\"\n    return mat.map(lambda row: [row[0]*2, row[1]-3])\n    ","user":"anonymous","dateUpdated":"2020-01-03T15:06:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578063767615_-667889372","id":"20200103-150247_1269393324","dateCreated":"2020-01-03T15:02:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1850","dateFinished":"2020-01-03T15:06:19+0000","dateStarted":"2020-01-03T15:06:19+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\"\"\"\nGraded cell\n\n1 point\n\"\"\"\nmatrix = [[1,3], [2,5], [8,9]]\nmatrix_rdd = sc.parallelize(matrix)\nresult_rdd = op1(sc, matrix_rdd)\n\nassert isinstance(result_rdd, RDD)\nassert result_rdd.collect() == [[2, 0], [4, 2], [16, 6]]","user":"anonymous","dateUpdated":"2020-01-03T15:06:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578062888013_-1083578171","id":"20200103-144808_705494330","dateCreated":"2020-01-03T14:48:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1011","dateFinished":"2020-01-03T15:06:21+0000","dateStarted":"2020-01-03T15:06:21+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\nQuestion 2\nSuppose we have a RDD containing sentences :\n\nsentences_rdd = sc.parallelize(['Hi everybody', 'My name is Fanilo', 'and your name is Antoine everybody'])\nCreate an operation .op2() which returns all the words in the rdd, after splitting each sentence by the whitespace character.","user":"anonymous","dateUpdated":"2020-01-03T15:07:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064010656_128249708","id":"20200103-150650_261949604","dateCreated":"2020-01-03T15:06:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2267","dateFinished":"2020-01-03T15:07:32+0000","dateStarted":"2020-01-03T15:07:32+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Question 2<br/>Suppose we have a RDD containing sentences :</p>\n<p>sentences_rdd = sc.parallelize([&lsquo;Hi everybody&rsquo;, &lsquo;My name is Fanilo&rsquo;, &lsquo;and your name is Antoine everybody&rsquo;])<br/>Create an operation .op2() which returns all the words in the rdd, after splitting each sentence by the whitespace character.</p>\n</div>"}]}},{"text":"%spark.pyspark\n\"Hello World !\".split(\" \")","user":"anonymous","dateUpdated":"2020-01-03T15:11:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064239714_-225918889","id":"20200103-151039_1648357671","dateCreated":"2020-01-03T15:10:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2465","dateFinished":"2020-01-03T15:11:10+0000","dateStarted":"2020-01-03T15:11:10+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"['Hello', 'World', '!']\n"}]}},{"text":"%spark.pyspark\ndef op2(sc, sentences):\n    \"\"\"\n    Return all words contained in the sentences.    \n    \"\"\"\n    return sentences.flatMap(lambda row: row.split(\" \"))","user":"anonymous","dateUpdated":"2020-01-03T15:13:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578063966750_-139208514","id":"20200103-150606_346200096","dateCreated":"2020-01-03T15:06:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2121","dateFinished":"2020-01-03T15:13:54+0000","dateStarted":"2020-01-03T15:13:54+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\n\"\"\"\nGraded cell\n\n1 point\n\"\"\"\nsentences_rdd = sc.parallelize(['Hi everybody', 'My name is Fanilo', 'and your name is Antoine everybody'])\nresult_rdd = op2(sc, sentences_rdd)\n\nassert isinstance(result_rdd, RDD)\nresult_rdd_collected = result_rdd.collect()\nassert result_rdd_collected == ['Hi', 'everybody', 'My', 'name', 'is', 'Fanilo', 'and', 'your', 'name', 'is', 'Antoine', 'everybody'], \"But have instead %s\" % str(result_rdd_collected)","user":"anonymous","dateUpdated":"2020-01-03T15:13:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064032546_1192901078","id":"20200103-150712_1040420392","dateCreated":"2020-01-03T15:07:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2339","dateFinished":"2020-01-03T15:13:56+0000","dateStarted":"2020-01-03T15:13:56+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n.filter() transformation\nThe .filter(function) transformation let's us filter elements verify a certain function.\n\nQuestion 3\nSuppose we have a RDD containing numbers.\n\nCreate an operation .op3() which returns all the odd numbers.","user":"anonymous","dateUpdated":"2020-01-03T15:14:22+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064454530_944905003","id":"20200103-151414_1316446883","dateCreated":"2020-01-03T15:14:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2743","dateFinished":"2020-01-03T15:14:22+0000","dateStarted":"2020-01-03T15:14:22+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>.filter() transformation<br/>The .filter(function) transformation let&rsquo;s us filter elements verify a certain function.</p>\n<p>Question 3<br/>Suppose we have a RDD containing numbers.</p>\n<p>Create an operation .op3() which returns all the odd numbers.</p>\n</div>"}]}},{"text":"%spark.pyspark\nsc.parallelize(range(20)).filter(lambda num: num > 5).collect()\n","user":"anonymous","dateUpdated":"2020-01-03T15:15:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064282503_-1293246657","id":"20200103-151122_267710881","dateCreated":"2020-01-03T15:11:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2593","dateFinished":"2020-01-03T15:15:10+0000","dateStarted":"2020-01-03T15:15:10+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"}]}},{"text":"%spark.pyspark\ndef op3(sc, numbers):\n    \"\"\"\n    Return all numbers contained in the RDD that are odd.    \n    \"\"\"\n    return numbers.filter(lambda x: x%2!=0)","user":"anonymous","dateUpdated":"2020-01-03T15:17:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064477082_-1344113908","id":"20200103-151437_185423936","dateCreated":"2020-01-03T15:14:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2854","dateFinished":"2020-01-03T15:17:46+0000","dateStarted":"2020-01-03T15:17:46+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\n\"\"\"\nGraded cell\n\n1 point\n\"\"\"\nnumbers = [1,2,3,4,5,6,7,8,9]\nnumbers_rdd = sc.parallelize(numbers)\nresult_rdd = op3(sc, numbers_rdd)\n\nassert isinstance(result_rdd, RDD)\nassert result_rdd.collect() == [1,3,5,7,9]","user":"anonymous","dateUpdated":"2020-01-03T15:17:47+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064490273_361487980","id":"20200103-151450_618054240","dateCreated":"2020-01-03T15:14:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2954","dateFinished":"2020-01-03T15:17:47+0000","dateStarted":"2020-01-03T15:17:47+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n.reduce() operation\nThe .reduce(function) transformation reduces all elements of the RDD into one using a specific method.\n\nDo take note that, as in the Hadoop ecosystem, the function used to reduce the dataset should be associative and commutative.\n\nQuestion 4\nSuppose we have a RDD containing numbers.\n\nCreate an operation .op4() which returns the sum of all squared odd numbers in the RDD, using the .reduce() operation.\n\nHint: now's a good time to tell you that chaining transformations is possible...","user":"anonymous","dateUpdated":"2020-01-03T15:18:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064682898_1400475101","id":"20200103-151802_1344454249","dateCreated":"2020-01-03T15:18:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3243","dateFinished":"2020-01-03T15:18:12+0000","dateStarted":"2020-01-03T15:18:12+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>.reduce() operation<br/>The .reduce(function) transformation reduces all elements of the RDD into one using a specific method.</p>\n<p>Do take note that, as in the Hadoop ecosystem, the function used to reduce the dataset should be associative and commutative.</p>\n<p>Question 4<br/>Suppose we have a RDD containing numbers.</p>\n<p>Create an operation .op4() which returns the sum of all squared odd numbers in the RDD, using the .reduce() operation.</p>\n<p>Hint: now&rsquo;s a good time to tell you that chaining transformations is possible&hellip;</p>\n</div>"}]}},{"text":"%spark.pyspark\nsc.parallelize(range(4)).reduce(lambda x,y: x+y)","user":"anonymous","dateUpdated":"2020-01-03T15:18:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064656685_-603256834","id":"20200103-151736_1891095557","dateCreated":"2020-01-03T15:17:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3123","dateFinished":"2020-01-03T15:18:27+0000","dateStarted":"2020-01-03T15:18:27+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"6\n"}]}},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2020-01-03T15:26:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578065208308_1644262326","id":"20200103-152648_467547637","dateCreated":"2020-01-03T15:26:48+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3748"},{"text":"%spark.pyspark\ndef op4(sc, numbers):\n    \"\"\"\n    Return the sum of all squared odd numbers.   \n    \"\"\"\n    return op3(sc, numbers).map(lambda x: x**2).reduce(lambda x,y: x+y)","user":"anonymous","dateUpdated":"2020-01-03T15:29:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064707658_1752034203","id":"20200103-151827_1853661681","dateCreated":"2020-01-03T15:18:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3339","dateFinished":"2020-01-03T15:29:56+0000","dateStarted":"2020-01-03T15:29:56+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\"\"\"\nGraded cell\n\n1 point\n\"\"\"\nnumbers = range(100)\nnumbers_rdd = sc.parallelize(numbers)\nresult = op4(sc, numbers_rdd)\n\nassert result == 166650, \"But have instead %d\" % result","user":"anonymous","dateUpdated":"2020-01-03T15:29:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064728019_-537956327","id":"20200103-151848_743675421","dateCreated":"2020-01-03T15:18:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3442","dateFinished":"2020-01-03T15:29:58+0000","dateStarted":"2020-01-03T15:29:58+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\nPart C - Paired RDDs\nIf you recall the classic MapReduce paradigm, you were dealing with key/value pairs to reduce your data in a distributed manner. We define a pair as a tuple of two elements, the first element being the key and the second the value.\n\nKey/value pairs are good for solving many problems efficiently in a parallel fashion so let us delve into them.\n\npairs = [('b', 3), ('d', 4), ('a', 6), ('f', 1), ('e', 2)]\npairs_rdd = sc.parallelize(pairs)\nreduceByKey\nThe .reduceByKey() method works in a similar way to the .reduce(), but it performs a reduction on a key-by-key basis.\n\nQuestion\nTime for the classic Hello world question !","user":"anonymous","dateUpdated":"2020-01-03T15:30:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578065417683_-1728526545","id":"20200103-153017_781640119","dateCreated":"2020-01-03T15:30:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3895","dateFinished":"2020-01-03T15:30:28+0000","dateStarted":"2020-01-03T15:30:28+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Part C - Paired RDDs<br/>If you recall the classic MapReduce paradigm, you were dealing with key/value pairs to reduce your data in a distributed manner. We define a pair as a tuple of two elements, the first element being the key and the second the value.</p>\n<p>Key/value pairs are good for solving many problems efficiently in a parallel fashion so let us delve into them.</p>\n<p>pairs = [(&lsquo;b&rsquo;, 3), (&lsquo;d&rsquo;, 4), (&lsquo;a&rsquo;, 6), (&lsquo;f&rsquo;, 1), (&lsquo;e&rsquo;, 2)]<br/>pairs_rdd = sc.parallelize(pairs)<br/>reduceByKey<br/>The .reduceByKey() method works in a similar way to the .reduce(), but it performs a reduction on a key-by-key basis.</p>\n<p>Question<br/>Time for the classic Hello world question !</p>\n</div>"}]}},{"text":"%spark.pyspark\nsc.parallelize(range(10)).map(lambda num: (num % 2, num)).reduceByKey(lambda x,y: x+y).collect()","user":"anonymous","dateUpdated":"2020-01-03T15:31:01+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578064732994_58647993","id":"20200103-151852_1497635419","dateCreated":"2020-01-03T15:18:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3520","dateFinished":"2020-01-03T15:31:02+0000","dateStarted":"2020-01-03T15:31:02+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[(0, 20), (1, 25)]\n"}]}},{"text":"%spark.pyspark\ndef wordcount(sc, sentences):\n    \"\"\"\n    Given a RDD of sentences, return the wordcount, after splitting sentences per whitespace.\n    \"\"\"\n    return op2(sc, sentences).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)","user":"anonymous","dateUpdated":"2020-01-03T15:36:47+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578065456112_-988380544","id":"20200103-153056_1748864756","dateCreated":"2020-01-03T15:30:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3991","dateFinished":"2020-01-03T15:36:47+0000","dateStarted":"2020-01-03T15:36:47+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\"\"\"\nGraded cell\n\n4 points\n\"\"\"\nsentences_rdd = sc.parallelize(['Hi everybody', 'My name is Fanilo', 'and your name is Antoine everybody'])\nresult_rdd = wordcount(sc, sentences_rdd)\n\nassert isinstance(result_rdd, RDD)\ncollected = result_rdd.collect()\nfor c in collected:  # This way, we do not depend on task resolution order\n    assert c in [\n    ('Hi', 1),\n    ('everybody', 2),\n    ('My', 1),\n    ('name', 2),\n    ('is', 2),\n    ('Fanilo', 1),\n    ('and', 1),\n    ('your', 1),\n    ('Antoine', 1)\n]","user":"anonymous","dateUpdated":"2020-01-03T15:46:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578065464124_609400080","id":"20200103-153104_1294103037","dateCreated":"2020-01-03T15:31:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4100","dateFinished":"2020-01-03T15:45:59+0000","dateStarted":"2020-01-03T15:45:59+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\njoin\nThe .join() method joins two RDD of pairs together on their key element.\n\nQuestion\nLet's give ourselves a student-gender RDD and a student-grade RDD. Compute the mean grade for each gender.\n\nHint: this is a long exercise. Remember that the mean for a gender equals the sum of all grades divided by the count of the number of grades. You already know how to sum by key, and you can use the countByKey() function for returning a hashmap of gender to count of grades, then use that hashmap inside a map function to divide. Good luck !","user":"anonymous","dateUpdated":"2020-01-03T15:46:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578066400594_-763589682","id":"20200103-154640_1452668229","dateCreated":"2020-01-03T15:46:40+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4456"},{"text":"%spark.pyspark\ngenders_rdd = sc.parallelize([('1', 'M'), ('2', 'M'), ('3', 'F'), ('4', 'F'), ('5', 'F'), ('6', 'M')])\ngrades_rdd = sc.parallelize([('1', 5), ('2', 12), ('3', 7), ('4', 18), ('5', 9), ('6', 5)])\n\ngenders_rdd.join(grades_rdd).collect()","user":"anonymous","dateUpdated":"2020-01-03T15:46:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578065476218_21164719","id":"20200103-153116_1590748687","dateCreated":"2020-01-03T15:31:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4175","dateFinished":"2020-01-03T15:47:00+0000","dateStarted":"2020-01-03T15:46:59+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[('4', ('F', 18)), ('1', ('M', 5)), ('5', ('F', 9)), ('3', ('F', 7)), ('6', ('M', 5)), ('2', ('M', 12))]\n"}]}},{"text":"%spark.pyspark\ndef mean_grade_per_gender(sc, genders, grades):\n    \"\"\"\n    Given a RDD of studentID to grades and studentID to gender, compute mean grade for each gender returned as paired RDD.\n    Assume all studentIDs are present in both RDDs, making inner join possible, no need to check that.\n    \"\"\"\n    joined_rdd = genders.join(grades)\n    gender_grade_rdd = joined_rdd.map(lambda x, y: y)\n    gender_count_rdd = gender_grade_rdd.countByKey()\n    gender_summed_rdd = gender_grade_rdd.reduceByKey(lambda x, y: x+y)\n    mean_gender_rdd = gender_summed_rdd.join(gender_count_rdd).map(lambda x, y: (x, y[0]/y[1]))  # TODO: May be possible to do easier\n    \n    return mean_gender_rdd","user":"anonymous","dateUpdated":"2020-01-03T16:02:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578066419875_1280943462","id":"20200103-154659_403220468","dateCreated":"2020-01-03T15:46:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4534","dateFinished":"2020-01-03T16:01:31+0000","dateStarted":"2020-01-03T16:01:31+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\"\"\"\nGraded cell\n\n4 points\n\"\"\"\ngenders_rdd = sc.parallelize([('1', 'M'), ('2', 'M'), ('3', 'F'), ('4', 'F'), ('5', 'F'), ('6', 'M')])\ngrades_rdd = sc.parallelize([('1', 5), ('2', 12), ('3', 7), ('4', 18), ('5', 9), ('6', 5)])\n\nresult_rdd = mean_grade_per_gender(sc, genders_rdd, grades_rdd)\nassert isinstance(result_rdd, RDD)\nassert result_rdd.collect() == [('M', 7.333333333333333), ('F', 11.333333333333334)]","user":"anonymous","dateUpdated":"2020-01-03T16:01:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578066445079_229539410","id":"20200103-154725_1990396711","dateCreated":"2020-01-03T15:47:25+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4634","dateFinished":"2020-01-03T16:01:33+0000","dateStarted":"2020-01-03T16:01:32+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 52.0 failed 4 times, most recent failure: Lost task 0.3 in stage 52.0 (TID 238, 172.31.41.3, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 839, in func\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 1253, in countPartition\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 839, in func\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 1253, in countPartition\n  File \"/home/ubuntu/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n', JavaObject id=o767), <traceback object at 0x7f90a372ed88>)"}]}},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2020-01-03T15:47:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578066449135_1364862375","id":"20200103-154729_625465698","dateCreated":"2020-01-03T15:47:29+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4721"}],"name":"PySpark-Introduction","id":"2EVVYC6YR","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}