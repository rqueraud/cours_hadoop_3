{"paragraphs":[{"text":"%md\n# Introduction\n\nDans ce Notebook, nous allons apprendre à utiliser la notion RDD de PySpark pour le calcul distribué. Ce notebook est inspiré de https://github.com/andfanilo/pyspark-tutorial.\n\nDe la même manière que le Notebook du TP précédent, vous disposez de cellules dans lesquelles vous pouvez insérer du code (indiquées explicitement dans les commentaires). Vous pouvez ensuite valider l'execution de votre cellule en executant la cellule de test à droite de votre cellule (Pensez bien à executer votre cellule avant d'executer la cellule de test pour mettre votre fonction à jour!)","user":"anonymous","dateUpdated":"2020-01-06T09:03:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578298878146_-649668411","id":"20200106-082118_1023595190","dateCreated":"2020-01-06T08:21:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9807","dateFinished":"2020-01-06T09:03:00+0000","dateStarted":"2020-01-06T09:03:00+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Introduction</h1>\n<p>Dans ce Notebook, nous allons apprendre à utiliser la notion RDD de PySpark pour le calcul distribué. Ce notebook est inspiré de <a href=\"https://github.com/andfanilo/pyspark-tutorial\">https://github.com/andfanilo/pyspark-tutorial</a>.</p>\n<p>De la même manière que le Notebook du TP précédent, vous disposez de cellules dans lesquelles vous pouvez insérer du code (indiquées explicitement dans les commentaires). Vous pouvez ensuite valider l&rsquo;execution de votre cellule en executant la cellule de test à droite de votre cellule (Pensez bien à executer votre cellule avant d&rsquo;executer la cellule de test pour mettre votre fonction à jour!)</p>\n</div>"}]}},{"text":"%md\n# Problème 0\n\nLa fonction `somme` suivante prend en entrée un entier `a` et un entier `b` et retourne la somme de `a` et de `b`.","user":"anonymous","dateUpdated":"2020-01-06T08:20:03+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578298653947_-1263741418","id":"20200106-081733_673841138","dateCreated":"2020-01-06T08:17:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9391","dateFinished":"2020-01-06T08:20:03+0000","dateStarted":"2020-01-06T08:20:03+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Problème 0</h1>\n<p>La fonction <code>somme</code> suivante prend en entrée un entier <code>a</code> et un entier <code>b</code> et retourne la somme de <code>a</code> et de <code>b</code>.</p>\n</div>"}]}},{"text":"%spark.pyspark\ndef somme(a, b):\n    return a + b","user":"anonymous","dateUpdated":"2020-01-06T08:20:09+0000","config":{"colWidth":9,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578298712893_-163913815","id":"20200106-081832_1382362010","dateCreated":"2020-01-06T08:18:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9463","dateFinished":"2020-01-06T08:20:09+0000","dateStarted":"2020-01-06T08:20:09+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\nassert somme(1, 2) == 3","user":"anonymous","dateUpdated":"2020-01-06T08:20:10+0000","config":{"colWidth":3,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578298723211_1615229488","id":"20200106-081843_1552130931","dateCreated":"2020-01-06T08:18:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9544","dateFinished":"2020-01-06T08:20:10+0000","dateStarted":"2020-01-06T08:20:10+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n# Problème 1\n\nÉcrivez une function `carre` prenant en entrée un entier `a` et qui retourne le carre de `a`.","user":"anonymous","dateUpdated":"2020-01-06T08:20:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578298485927_-425552411","id":"20200106-081445_280912590","dateCreated":"2020-01-06T08:14:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9274","dateFinished":"2020-01-06T08:20:17+0000","dateStarted":"2020-01-06T08:20:17+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Problème 1</h1>\n<p>Écrivez une function <code>carre</code> prenant en entrée un entier <code>a</code> et qui retourne le carre de <code>a</code>.</p>\n</div>"}]}},{"text":"%spark.pyspark\n##### Insérer le code ici #####\ndef carre(a):\n    return a*a\n###############################","user":"anonymous","dateUpdated":"2020-01-06T08:56:47+0000","config":{"colWidth":9,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578062593673_866441061","id":"20200103-144313_2058655224","dateCreated":"2020-01-03T14:43:13+0000","dateStarted":"2020-01-06T08:56:47+0000","dateFinished":"2020-01-06T08:56:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6796"},{"text":"%spark.pyspark\nassert carre(2) == 4","user":"anonymous","dateUpdated":"2020-01-06T08:56:48+0000","config":{"colWidth":3,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578298426539_760783406","id":"20200106-081346_763855201","dateCreated":"2020-01-06T08:13:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9169","dateFinished":"2020-01-06T08:56:49+0000","dateStarted":"2020-01-06T08:56:49+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nLe RDD est la notion de base (la plus bas-niveau) de Spark. Des structures plus haut-niveau, tels que les Datasets/Dataframe proposent une abstraction du RDD pour travailler plus simplement avec de grandes quantités de données. \n\nLe RDD consiste en une collection d'éléments distribués sur les différents noeuds de notre cluster. Il y a deux manières d'en créer : depuis une liste python ou depuis un fichier de données. Nous explorerons ici le fonctionnement des RDD à partir de listes python, le fonctionnement restant le même avec les fichiers.\n\nOn peut générer un RDD depuis une liste python de la manière suivante :\n\n```python\nsc.parallelize([1, 2, 3]).collect()\n```\n\n`sc` est une variable déclarée par Spark représentant le *SparkContext*.  \n`parallelize` est une fonction permettant de créer un RDD à partir d'une liste python.  \n`collect` est une fonction permettant d'évaluer les calculs faits sur un RDD.\n\nLes RDD sont une structure proposant des calculs `paresseux`, c'est à dire que rien n'est évalué tant qu'on ne demande pas d'évaluer nos opérations pour obtenir un resultat.\n\n# Problème 2\n\nÉcrire une fonction `rdd_from_list` prenant en paramètre le *SparkContext* `sc` et un entier `n` qui retourne un RDD composé des `n` premiers entiers. Par exemple, `rdd_from_list(sc, 3)` retourne un rdd composé des valeurs 0, 1 et 2. Rappelez-vous de la fonction `range` vue au premier TP.","user":"anonymous","dateUpdated":"2020-01-06T08:56:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Le RDD est la notion de base (la plus bas-niveau) de Spark. Des structures plus haut-niveau, tels que les Datasets/Dataframe proposent une abstraction du RDD pour travailler plus simplement avec de grandes quantités de données. </p>\n<p>Le RDD consiste en une collection d&rsquo;éléments distribués sur les différents noeuds de notre cluster. Il y a deux manières d&rsquo;en créer : depuis une liste python ou depuis un fichier de données. Nous explorerons ici le fonctionnement des RDD à partir de listes python, le fonctionnement restant le même avec les fichiers.</p>\n<p>On peut générer un RDD depuis une liste python de la manière suivante :</p>\n<pre><code class=\"python\">sc.parallelize([1, 2, 3]).collect()\n</code></pre>\n<p><code>sc</code> est une variable déclarée par Spark représentant le <em>SparkContext</em>.<br/><code>parallelize</code> est une fonction permettant de créer un RDD à partir d&rsquo;une liste python.<br/><code>collect</code> est une fonction permettant d&rsquo;évaluer les calculs faits sur un RDD.</p>\n<p>Les RDD sont une structure proposant des calculs <code>paresseux</code>, c&rsquo;est à dire que rien n&rsquo;est évalué tant qu&rsquo;on ne demande pas d&rsquo;évaluer nos opérations pour obtenir un resultat.</p>\n<h1>Problème 2</h1>\n<p>Écrire une fonction <code>rdd_from_list</code> prenant en paramètre le <em>SparkContext</em> <code>sc</code> et un entier <code>n</code> qui retourne un RDD composé des <code>n</code> premiers entiers. Par exemple, <code>rdd_from_list(sc, 3)</code> retourne un rdd composé des valeurs 0, 1 et 2. Rappelez-vous de la fonction <code>range</code> vue au premier TP.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578063436357_-990651046","id":"20200103-145716_582053025","dateCreated":"2020-01-03T14:57:16+0000","dateStarted":"2020-01-06T08:56:56+0000","dateFinished":"2020-01-06T08:56:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6797"},{"text":"%spark.pyspark\n\nfrom pyspark.rdd import RDD\n\nsc.parallelize([1, 2, 3]).collect()  # L'exemple\n\ndef rdd_from_list(sc, n):\n    \"\"\"\n    Return a RDD consisting of elements from 0 to n (excluded). \n    For now we assume we will always get n > 1, no need to test for the exception nor raise an Exception.\n    \"\"\"\n    ##### Insérer le code ici #####\n    \n    ###############################\n","user":"anonymous","dateUpdated":"2020-01-06T09:00:28+0000","config":{"colWidth":9,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578062704211_-1990598944","id":"20200103-144504_1162212700","dateCreated":"2020-01-03T14:45:04+0000","dateStarted":"2020-01-06T08:59:29+0000","dateFinished":"2020-01-06T08:59:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6798"},{"text":"%spark.pyspark\nresult_rdd = rdd_from_list(sc, 5)\n\nassert isinstance(result_rdd, RDD)\nassert result_rdd.collect() == [0, 1, 2, 3, 4]","user":"anonymous","dateUpdated":"2020-01-06T08:59:55+0000","config":{"colWidth":3,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578300897822_1902080354","id":"20200106-085457_1784808015","dateCreated":"2020-01-06T08:54:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9958","dateFinished":"2020-01-06T08:59:55+0000","dateStarted":"2020-01-06T08:59:55+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nLes RDD ont deux types d'opérations parallèles:\n  - Les transformations: Qui retournent des pointeurs vers de nouveaux RDD sans les calculers (de manière paresseuse).\n  - Les actions: Qui retournent les valeurs à l'acteur ayant demandé le calcul, après les avoirs executés sur les différents noeuds. `collect` est une de ces actions qui retourne tous les éléments du RDD à l'acteur ayant demandé le calcul.\n\nParmis les transformations, `.map(function)` applique la fonction à chacun des éléments du RDD. Le code suivant retourne ainsi `[1, 2]`:\n\n```python\nsc.parallelize([[1,3], [2,9]]).map(lambda row: row[0]).collect()\n```\n\n# Problème 3\n\nSupposons que nous ayons un RDD contenant des listes de 2 éléments :\n\n```python\nmatrix = [[1,3], [2,5], [8,9]]\nmatrix_rdd = sc.parallelize(matrix)\nThis data structure is reminiscent of a matrix.\n```\n\nCréez une fonction `pb3()` qui multiplie la première colonne (ou la première coordonnée de chaque élément) de la matrice par 2, et enlève 3 à la seconde colonne.\n\n","user":"anonymous","dateUpdated":"2020-01-06T09:56:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Les RDD ont deux types d&rsquo;opérations parallèles:<br/> - Les transformations: Qui retournent des pointeurs vers de nouveaux RDD sans les calculers (de manière paresseuse).<br/> - Les actions: Qui retournent les valeurs à l&rsquo;acteur ayant demandé le calcul, après les avoirs executés sur les différents noeuds. <code>collect</code> est une de ces actions qui retourne tous les éléments du RDD à l&rsquo;acteur ayant demandé le calcul.</p>\n<p>Parmis les transformations, <code>.map(function)</code> applique la fonction à chacun des éléments du RDD. Le code suivant retourne ainsi <code>[1, 3]</code>:</p>\n<pre><code class=\"python\">sc.parallelize([[1,3], [2,9]]).map(lambda row: row[0]).collect()\n</code></pre>\n<h1>Problème 3</h1>\n<p>Supposons que nous ayons un RDD contenant des listes de 2 éléments :</p>\n<pre><code class=\"python\">matrix = [[1,3], [2,5], [8,9]]\nmatrix_rdd = sc.parallelize(matrix)\nThis data structure is reminiscent of a matrix.\n</code></pre>\n<p>Créez une fonction <code>pb3()</code> qui multiplie la première colonne (ou la première coordonnée de chaque élément) de la matrice par 2, et enlève 3 à la seconde colonne.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578063770088_-1082280858","id":"20200103-150250_1585400570","dateCreated":"2020-01-03T15:02:50+0000","dateStarted":"2020-01-06T09:56:58+0000","dateFinished":"2020-01-06T09:56:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6799"},{"text":"%spark.pyspark\n\n\ndef pb3(sc, mat):\n    \"\"\"\n    Multiply the first coordinate by 2, remove 3 to the second\n    \"\"\"\n    ##### Insérer le code ici #####\n    \n    ###############################\n    \n    ","user":"anonymous","dateUpdated":"2020-01-06T09:14:30+0000","config":{"colWidth":9,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578063767615_-667889372","id":"20200103-150247_1269393324","dateCreated":"2020-01-03T15:02:47+0000","dateStarted":"2020-01-06T09:13:47+0000","dateFinished":"2020-01-06T09:13:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6801"},{"text":"%spark.pyspark\nmatrix = [[1,3], [2,5], [8,9]]\nmatrix_rdd = sc.parallelize(matrix)\nresult_rdd = pb3(sc, matrix_rdd)\n\nassert isinstance(result_rdd, RDD)\nassert result_rdd.collect() == [[2, 0], [4, 2], [16, 6]]","user":"anonymous","dateUpdated":"2020-01-06T09:13:59+0000","config":{"colWidth":3,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578062888013_-1083578171","id":"20200103-144808_705494330","dateCreated":"2020-01-03T14:48:08+0000","dateStarted":"2020-01-06T09:13:59+0000","dateFinished":"2020-01-06T09:14:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6802"},{"text":"%md\n\nUn autre type de transformation, `.flatMap(function)` applique la fonction à tous les éléments du RDD, puis fusionne les listes en une seule. Par exemple, le code suivant retourne `[2, 3, 4, 5]`:\n\n```python\nsc.parallelize([[1, 2], [3, 4]]).flatMap(lambda row: [row[0]+1, row[1]+1]).collect()\n```\n\n# Problème 4\n\nSupposons qu'on ait un RDD qui contient les phrases : `sentences_rdd = sc.parallelize(['bonne année', 'et bonne santé', 'à tous'])`\n\nCréez une fonction `pb4` qui retourne tous les mots dans le RDD.  \nPour rappel, la fonction python `split` permet de séparer une chaîne de caractère selon le caractère donné en entrée: `\"abracadabra\".split(\"c\")` retourne `['abra', 'adabra']`.","user":"anonymous","dateUpdated":"2020-01-06T09:25:57+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Un autre type de transformation, <code>.flatMap(function)</code> applique la fonction à tous les éléments du RDD, puis fusionne les listes en une seule. Par exemple, le code suivant retourne <code>[2, 3, 4, 5]</code>:</p>\n<pre><code class=\"python\">sc.parallelize([[1, 2], [3, 4]]).flatMap(lambda row: [row[0]+1, row[1]+1]).collect()\n</code></pre>\n<h1>Problème 4</h1>\n<p>Supposons qu&rsquo;on ait un RDD qui contient les phrases : <code>sentences_rdd = sc.parallelize([&#39;bonne année&#39;, &#39;et bonne santé&#39;, &#39;à tous&#39;])</code></p>\n<p>Créez une fonction <code>pb4</code> qui retourne tous les mots dans le RDD.<br/>Pour rappel, la fonction python <code>split</code> permet de séparer une chaîne de caractère selon le caractère donné en entrée: <code>&quot;abracadabra&quot;.split(&quot;c&quot;)</code> retourne <code>[&#39;abra&#39;, &#39;adabra&#39;]</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578064010656_128249708","id":"20200103-150650_261949604","dateCreated":"2020-01-03T15:06:50+0000","dateStarted":"2020-01-06T09:25:57+0000","dateFinished":"2020-01-06T09:25:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6803"},{"text":"%spark.pyspark\ndef pb4(sc, sentences):\n    \"\"\"\n    Return all words contained in the sentences.    \n    \"\"\"\n    ##### Insérer le code ici #####\n    \n    ###############################","user":"anonymous","dateUpdated":"2020-01-06T09:33:51+0000","config":{"colWidth":5,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578063966750_-139208514","id":"20200103-150606_346200096","dateCreated":"2020-01-03T15:06:06+0000","dateStarted":"2020-01-06T09:33:36+0000","dateFinished":"2020-01-06T09:33:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6805"},{"text":"%spark.pyspark\n\n\"\"\"\nGraded cell\n\n1 point\n\"\"\"\nsentences_rdd = sc.parallelize(['bonne année', 'et bonne santé', 'à tous'])\nresult_rdd = pb4(sc, sentences_rdd)\n\nassert isinstance(result_rdd, RDD)\nresult_rdd_collected = result_rdd.collect()\nassert result_rdd_collected == ['bonne', 'année', 'et', 'bonne', 'santé', 'à', 'tous'], \"But have instead %s\" % str(result_rdd_collected)","user":"anonymous","dateUpdated":"2020-01-06T09:33:38+0000","config":{"colWidth":7,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578064032546_1192901078","id":"20200103-150712_1040420392","dateCreated":"2020-01-03T15:07:12+0000","dateStarted":"2020-01-06T09:33:38+0000","dateFinished":"2020-01-06T09:33:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6806"},{"text":"%md\nLa transformation `.filter(function)` nous permet de filtrer les éléments vérifiant la fonction. Par exemple `sc.parallelize(range(20)).filter(lambda num: num > 5).collect()` ne retourne que les éléments strictement supérieurs à 5.\n\n# Problème 5\n\nSupposons qu'on ait un RDD qui contienne des nombers.\n\nCréez une fonction `pb5` qui retourne uniquement les nombres impairs.  \nPour rappel, l'opérateur python modulo (`%`) retourne le reste de la division entière. Par exemple `10 % 3` retourne `1`.","user":"anonymous","dateUpdated":"2020-01-06T09:56:52+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>La transformation <code>.filter(function)</code> nous permet de filtrer les éléments vérifiant la fonction. Par exemple <code>sc.parallelize(range(20)).filter(lambda num: num &gt; 5).collect()</code> ne retourne que les éléments strictement supérieurs à 5.</p>\n<h1>Problème 5</h1>\n<p>Supposons qu&rsquo;on ait un RDD qui contienne des nombers.</p>\n<p>Créez une fonction <code>pb5</code> qui retourne uniquement les nombres impairs.<br/>Pour rappel, l&rsquo;opérateur python modulo (<code>%</code>) retourne le reste de la division entière. Par exemple <code>10 % 3</code> retourne <code>1</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578064454530_944905003","id":"20200103-151414_1316446883","dateCreated":"2020-01-03T15:14:14+0000","dateStarted":"2020-01-06T09:56:52+0000","dateFinished":"2020-01-06T09:56:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6807"},{"text":"%spark.pyspark\ndef pb5(sc, numbers):\n    \"\"\"\n    Return all numbers contained in the RDD that are odd.    \n    \"\"\"\n    ##### Insérer le code ici #####\n    \n    ###############################","user":"anonymous","dateUpdated":"2020-01-06T09:47:42+0000","config":{"colWidth":7,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578064477082_-1344113908","id":"20200103-151437_185423936","dateCreated":"2020-01-03T15:14:37+0000","dateStarted":"2020-01-06T09:47:19+0000","dateFinished":"2020-01-06T09:47:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6809"},{"text":"%spark.pyspark\n\nnumbers = [1,2,3,4,5,6,7,8,9]\nnumbers_rdd = sc.parallelize(numbers)\nresult_rdd = pb5(sc, numbers_rdd)\n\nassert isinstance(result_rdd, RDD)\nassert result_rdd.collect() == [1,3,5,7,9]","user":"anonymous","dateUpdated":"2020-01-06T09:47:20+0000","config":{"colWidth":5,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578064490273_361487980","id":"20200103-151450_618054240","dateCreated":"2020-01-03T15:14:50+0000","dateStarted":"2020-01-06T09:47:20+0000","dateFinished":"2020-01-06T09:47:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6810"},{"text":"%md\n\nLa transformation `.reduce(function)` permet de reduire tous les éléments du RDD en un élément en utilisant la fonction définie. Cette fonction doit être associative et commutative. Par exemple:\n\n```python\nsc.parallelize(range(4)).reduce(lambda x,y: x+y)  # Retourne 6\n```\n\n# Problème 6\n\nOn a un RDD contenant des nombres. Créez une fonction `pb6` qui retourne la somme de tous les nombres impairs au carré du RDD.\n\nNotes:\n  - Pensez à enchainer les appels de fonction.\n  - Pensez à reutiliser la fonction créée précédemment.","user":"anonymous","dateUpdated":"2020-01-06T09:56:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>La transformation <code>.reduce(function)</code> permet de reduire tous les éléments du RDD en un élément en utilisant la fonction définie. Cette fonction doit être associative et commutative. Par exemple:</p>\n<pre><code class=\"python\">sc.parallelize(range(4)).reduce(lambda x,y: x+y)  # Retourne 6\n</code></pre>\n<h1>Problème 6</h1>\n<p>On a un RDD contenant des nombres. Créez une fonction <code>pb6</code> qui retourne la somme de tous les nombres impairs au carré du RDD.</p>\n<p>Notes:<br/> - Pensez à enchainer les appels de fonction.<br/> - Pensez à reutiliser la fonction créée précédemment.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578064682898_1400475101","id":"20200103-151802_1344454249","dateCreated":"2020-01-03T15:18:02+0000","dateStarted":"2020-01-06T09:56:45+0000","dateFinished":"2020-01-06T09:56:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6811"},{"text":"%spark.pyspark\ndef pb6(sc, numbers):\n    \"\"\"\n    Return the sum of all squared odd numbers.   \n    \"\"\"\n    ##### Insérer le code ici #####\n    \n    ###############################","user":"anonymous","dateUpdated":"2020-01-06T10:09:20+0000","config":{"colWidth":7,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578064707658_1752034203","id":"20200103-151827_1853661681","dateCreated":"2020-01-03T15:18:27+0000","dateStarted":"2020-01-06T09:55:49+0000","dateFinished":"2020-01-06T09:55:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6814"},{"text":"%spark.pyspark\nnumbers = range(100)\nnumbers_rdd = sc.parallelize(numbers)\nresult = pb6(sc, numbers_rdd)\n\nassert result == 166650, \"But have instead %d\" % result","user":"anonymous","dateUpdated":"2020-01-06T09:56:01+0000","config":{"colWidth":5,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578064728019_-537956327","id":"20200103-151848_743675421","dateCreated":"2020-01-03T15:18:48+0000","dateStarted":"2020-01-06T09:56:01+0000","dateFinished":"2020-01-06T09:56:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6815"},{"text":"%md\n\nLes RDD peuvent être constitués d'une liste de paires clé/valeurs. Le premier élément est la clé, et le second est la valeur.\n\nLa transformation `reduceByKey`, sur le même principe que le `reduce`, permet par exemple de reduire selon la clé:\n\n```python\nsc.parallelize(range(10))\n    .map(lambda num: (num % 2, num))\n    .reduceByKey(lambda x,y: x+y)\n    .collect()\n```\n\nL'exemple ci-dessus retourne `[(0, 20), (1, 25)]`.\n\n# Problème 7\n\nCréez une fonction `pb7` prenant un RDD de phrases, et retournant un RDD constitué du compte de chaque mot.","user":"anonymous","dateUpdated":"2020-01-06T10:13:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Les RDD peuvent être constitués d&rsquo;une liste de paires clé/valeurs. Le premier élément est la clé, et le second est la valeur.</p>\n<p>La transformation <code>reduceByKey</code>, sur le même principe que le <code>reduce</code>, permet par exemple de reduire selon la clé:</p>\n<pre><code class=\"python\">sc.parallelize(range(10))\n    .map(lambda num: (num % 2, num))\n    .reduceByKey(lambda x,y: x+y)\n    .collect()\n</code></pre>\n<p>L&rsquo;exemple ci-dessus retourne <code>[(0, 20), (1, 25)]</code>.</p>\n<h1>Problème 7</h1>\n<p>Créez une fonction <code>pb7</code> prenant un RDD de phrases, et retournant un RDD constitué du compte de chaque mot.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578065417683_-1728526545","id":"20200103-153017_781640119","dateCreated":"2020-01-03T15:30:17+0000","dateStarted":"2020-01-06T10:13:18+0000","dateFinished":"2020-01-06T10:13:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6816"},{"text":"%spark.pyspark\ndef pb7(sc, sentences):\n    \"\"\"\n    Given a RDD of sentences, return the wordcount, after splitting sentences per whitespace.\n    \"\"\"\n    ##### Insérer le code ici #####\n    \n    ###############################","user":"anonymous","dateUpdated":"2020-01-06T10:09:24+0000","config":{"colWidth":5,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578065456112_-988380544","id":"20200103-153056_1748864756","dateCreated":"2020-01-03T15:30:56+0000","dateStarted":"2020-01-06T10:08:47+0000","dateFinished":"2020-01-06T10:08:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6818"},{"text":"%spark.pyspark\nsentences_rdd = sc.parallelize(['bonne année', 'et bonne santé', 'à tous'])\nresult_rdd = pb7(sc, sentences_rdd)\n\nassert isinstance(result_rdd, RDD)\ncollected = result_rdd.collect()\nfor c in collected:  # This way, we do not depend on task resolution order\n    assert c in [\n    ('bonne', 2),\n    ('année', 1),\n    ('et', 1),\n    ('santé', 1),\n    ('à', 1),\n    ('tous', 1)\n]","user":"anonymous","dateUpdated":"2020-01-06T10:08:49+0000","config":{"colWidth":7,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578065464124_609400080","id":"20200103-153104_1294103037","dateCreated":"2020-01-03T15:31:04+0000","dateStarted":"2020-01-06T10:08:49+0000","dateFinished":"2020-01-06T10:08:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6819"},{"text":"%md\n\nLa transformation `.join` permet de joindre deux RDD avec des pairs de clé/valeur sur leur élément clé.  \nLa transformation `.countByKey` permet de retourner le compte par clé.\n\n# Problème 8\n\nOn prend un RDD d'étudiant-genre et un RDD d'étudiant-note. Créez une fonction `pb8` qui, à partir de ces 2 RDD, calcule la note moyenne de chaque genre.\n\nC'est un exercice assez long qui demande d'enchainer plusieurs opérations. N'hésitez pas à stocker vos resultats intermediaires dans des variables pour en valider le contenu. Bon courage !","user":"anonymous","dateUpdated":"2020-01-06T10:18:47+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578066400594_-763589682","id":"20200103-154640_1452668229","dateCreated":"2020-01-03T15:46:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6820","dateFinished":"2020-01-06T10:18:47+0000","dateStarted":"2020-01-06T10:18:47+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>La transformation <code>.join</code> permet de joindre deux RDD avec des pairs de clé/valeur sur leur élément clé.<br/>La transformation <code>.countByKey</code> permet de retourner le compte par clé.</p>\n<h1>Problème 8</h1>\n<p>On prend un RDD d&rsquo;étudiant-genre et un RDD d&rsquo;étudiant-note. Créez une fonction <code>pb8</code> qui, à partir de ces 2 RDD, calcule la note moyenne de chaque genre.</p>\n<p>C&rsquo;est un exercice assez long qui demande d&rsquo;enchainer plusieurs opérations. N&rsquo;hésitez pas à stocker vos resultats intermediaires dans des variables pour en valider le contenu. Bon courage !</p>\n</div>"}]}},{"text":"%spark.pyspark\ndef pb8(sc, genders, grades):\n    \"\"\"\n    Given a RDD of studentID to grades and studentID to gender, compute mean grade for each gender returned as paired RDD.\n    Assume all studentIDs are present in both RDDs, making inner join possible, no need to check that.\n    \"\"\"\n    ##### Insérer le code ici #####\n    \n    ###############################","user":"anonymous","dateUpdated":"2020-01-06T10:20:39+0000","config":{"colWidth":8,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1578066419875_1280943462","id":"20200103-154659_403220468","dateCreated":"2020-01-03T15:46:59+0000","dateStarted":"2020-01-06T10:20:07+0000","dateFinished":"2020-01-06T10:20:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6822"},{"text":"%spark.pyspark\ngenders_rdd = sc.parallelize([('1', 'M'), ('2', 'M'), ('3', 'F'), ('4', 'F'), ('5', 'F'), ('6', 'M')])\ngrades_rdd = sc.parallelize([('1', 5), ('2', 12), ('3', 7), ('4', 18), ('5', 9), ('6', 5)])\n\nresult_rdd = pb8(sc, genders_rdd, grades_rdd)\nassert isinstance(result_rdd, RDD)\nassert result_rdd.collect() == [('M', 7.333333333333333), ('F', 11.333333333333334)]","user":"anonymous","dateUpdated":"2020-01-06T10:20:16+0000","config":{"colWidth":4,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578066445079_229539410","id":"20200103-154725_1990396711","dateCreated":"2020-01-03T15:47:25+0000","dateStarted":"2020-01-06T10:20:16+0000","dateFinished":"2020-01-06T10:20:17+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6823","errorMessage":""},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2020-01-03T15:47:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578066449135_1364862375","id":"20200103-154729_625465698","dateCreated":"2020-01-03T15:47:29+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6824"}],"name":"PySpark-Introduction","id":"2EVVYC6YR","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}